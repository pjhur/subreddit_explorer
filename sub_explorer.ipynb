{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit Explorer\n",
    "\n",
    "This project was originally completed using 97 million comments from September 2018. However, Git has a 100 MB file size limit. Therefore, I created a sample set of approximately 1 million comments and used it below.\n",
    "<br><br>\n",
    "The results are decent using the sample set (but not as good as the results from the full data set). On the plus side, the entire notebook finishes running in under 2 minutes on my machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark\n",
    "- I used a 4-core CPU with 16 GB of memory.\n",
    "- These settings may need to be adjusted for your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "conf = ps.SparkConf().setAll([('spark.executor.memory', '6g'), \n",
    "                                   ('spark.executor.cores', '3'),\n",
    "                                   ('spark.driver.memory','6g')])\n",
    "spark = ps.sql.SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sample comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sept = spark.read.csv('sample_sept.csv/', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "964524"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sept.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author_fullname: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_sept.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows where author_fullname = null\n",
    "- These are users where the account was deleted after the comment was created.\n",
    "- The sample comments file should not have any of these.\n",
    "- However, this is an important step if you are using your own source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_sept = sample_sept.filter(sample_sept.author_fullname != 'null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert subreddit_id to base 10\n",
    "- Original format is base 36 (0-9 and a-z)\n",
    "- New column is 'sr_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import conv\n",
    "from pyspark.sql.functions import substring\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substring is used to remove prefix 't5_'\n",
    "base10_df = sample_sept.withColumn('sr_id', \n",
    "            conv(substring(sample_sept.subreddit_id, 4, \n",
    "            8), 36, 10).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-----------------+------------+-------+\n",
      "|author_fullname|         author|        subreddit|subreddit_id|  sr_id|\n",
      "+---------------+---------------+-----------------+------------+-------+\n",
      "|       t2_dca4l|Doctah_Whoopass|             cars|    t5_2qhl2|4595078|\n",
      "|       t2_m9hec|     buggiegirl|            funny|    t5_2qh33|4594431|\n",
      "|       t2_qq3rl|     Aguas-chan|        AskReddit|    t5_2qh1i|4594374|\n",
      "|    t2_1qm8eh7n|  DenverNuggetz|interestingasfuck|    t5_2qhsa|4595338|\n",
      "|       t2_y99ws|        progwok|        AskReddit|    t5_2qh1i|4594374|\n",
      "+---------------+---------------+-----------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base10_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author_fullname: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- sr_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base10_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group rows to create rating column\n",
    "- Each row represents a comment\n",
    "- Multiple comments in a subreddit by the same person need to be grouped\n",
    "- Rating column represents the number of comments (per subreddit)\n",
    "- This is the format required by PySpark ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import lit\n",
    "grouped_df = base10_df.groupBy(base10_df.columns).agg(count(lit(1)).alias('rating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198312"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+---------+------------+-------+------+\n",
      "|author_fullname|           author|subreddit|subreddit_id|  sr_id|rating|\n",
      "+---------------+-----------------+---------+------------+-------+------+\n",
      "|       t2_5uyfq|         tman916x|      nba|    t5_2qo4s|4603564|    61|\n",
      "|       t2_99q54|         jmeshvrd|     tifu|    t5_2to41|4743505|     2|\n",
      "|       t2_frjqq|      GhostCheese|pokemongo|    t5_34jka|5250826|    32|\n",
      "|      t2_103ceu|         Shodan30| Konosuba|    t5_3c02n|5598815|     5|\n",
      "|    t2_1l3cv8h9|International_Way|AskReddit|    t5_2qh1i|4594374|   162|\n",
      "+---------------+-----------------+---------+------------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new author identifier column\n",
    "- The original base 36 author identifier will not work\n",
    "    - Many of them exceed the maximum integer size for PySpark ALS\n",
    "- We will instead create our own author identifier\n",
    "- I tried using monotonically_increasing_id() and ZipWithUniqueId()\n",
    "    - Both created ids larger than the maximum integer size for PySpark ALS\n",
    "    - (When using full data set of 97 million rows)\n",
    "- zipWithIndex() works but uses RDDs which adds additional steps\n",
    "- StringIndexer() is easiest solution that I found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import col\n",
    "stringIndexer = StringIndexer(inputCol=\"author\", outputCol=\"au_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_model = stringIndexer.fit(grouped_df)\n",
    "auth_df = si_model.transform(grouped_df).withColumn(\"au_id\", \n",
    "                                     col(\"au_id\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[author_fullname: string, author: string, subreddit: string, subreddit_id: string, sr_id: int, rating: bigint, au_id: int]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache here if you are doing further data exploration.\n",
    "auth_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+---------+------------+-------+------+-----+\n",
      "|author_fullname|           author|subreddit|subreddit_id|  sr_id|rating|au_id|\n",
      "+---------------+-----------------+---------+------------+-------+------+-----+\n",
      "|       t2_5uyfq|         tman916x|      nba|    t5_2qo4s|4603564|    61| 6488|\n",
      "|       t2_99q54|         jmeshvrd|     tifu|    t5_2to41|4743505|     2| 1397|\n",
      "|       t2_frjqq|      GhostCheese|pokemongo|    t5_34jka|5250826|    32| 1782|\n",
      "|      t2_103ceu|         Shodan30| Konosuba|    t5_3c02n|5598815|     5| 3508|\n",
      "|    t2_1l3cv8h9|International_Way|AskReddit|    t5_2qh1i|4594374|   162|  617|\n",
      "+---------------+-----------------+---------+------------+-------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "auth_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe for ALS input\n",
    "- ALS requires (user, item, rating) format\n",
    "- All three fields must be integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_df = auth_df.select(['au_id', 'sr_id', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- au_id: integer (nullable = true)\n",
      " |-- sr_id: integer (nullable = true)\n",
      " |-- rating: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "als_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run ALS implicit recommendation model\n",
    "https://spark.apache.org/docs/2.2.0/mllib-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "als = ALS()\n",
    "# This setting may help people with less memory\n",
    "# sc.setCheckpointDir('/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_fitted = als.trainImplicit(als_df.rdd, rank=75, iterations=5,\n",
    "                                  lambda_=0.01, alpha=0.01, seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get items (and users) matrices\n",
    "- PySpark ALS returns these in RDD format\n",
    "- First, we convert these into a Spark dataframe with nested array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_rdd = als_fitted.productFeatures()\n",
    "# users_rdd = als_fitted.userFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "\n",
    "def f(x):\n",
    "    d = {}\n",
    "    for i in range(len(x)):\n",
    "        d[str(i)] = x[i]\n",
    "    return d\n",
    "\n",
    "items_df = items_rdd.map(lambda x: Row(**f(x))).toDF()\n",
    "# users_df = users_rdd.map(lambda x: Row(**f(x))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|      0|                   1|\n",
      "+-------+--------------------+\n",
      "|4594300|[0.00937710888683...|\n",
      "|4594600|[-0.0023872260935...|\n",
      "|4594700|[-3.4862846951000...|\n",
      "|4596900|[-3.3788694418035...|\n",
      "|4598300|[-8.3739380352199...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert into a pandas dataframe as a true matrix\n",
    "- The matrix in the dataframe is nested within a single column\n",
    "- It's much easier to do matrix operations if we convert to a matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "items_pd = pd.DataFrame(np.row_stack(items_df.select('1').collect()))\n",
    "# users_pd = pd.DataFrame(np.row_stack(users_df.select('1').collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009377</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.021777</td>\n",
       "      <td>-0.004703</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>-0.000648</td>\n",
       "      <td>-0.002421</td>\n",
       "      <td>-0.009784</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>0.005625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>-0.009730</td>\n",
       "      <td>-0.004119</td>\n",
       "      <td>-0.007043</td>\n",
       "      <td>-0.002126</td>\n",
       "      <td>-0.005411</td>\n",
       "      <td>-0.009777</td>\n",
       "      <td>-0.016776</td>\n",
       "      <td>0.007655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.002387</td>\n",
       "      <td>-0.002470</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>-0.002490</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>-0.004600</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003803</td>\n",
       "      <td>0.005502</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>-0.002151</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.005457</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>-0.002091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000349</td>\n",
       "      <td>-0.004091</td>\n",
       "      <td>-0.002694</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>-0.000880</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>-0.002247</td>\n",
       "      <td>-0.002289</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.002538</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>-0.000951</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>0.003184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000338</td>\n",
       "      <td>-0.002806</td>\n",
       "      <td>-0.002333</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>-0.002035</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>-0.003581</td>\n",
       "      <td>-0.001602</td>\n",
       "      <td>-0.000692</td>\n",
       "      <td>-0.000991</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>-0.002380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000837</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>-0.000519</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>-0.000722</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001045</td>\n",
       "      <td>-0.000203</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>-0.001107</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>-0.000582</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>-0.000032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.009377  0.001945  0.021777 -0.004703  0.001998 -0.000648 -0.002421   \n",
       "1 -0.002387 -0.002470  0.002993  0.000047 -0.000001 -0.001413 -0.002490   \n",
       "2 -0.000349 -0.004091 -0.002694  0.000687 -0.000880 -0.001034 -0.002247   \n",
       "3 -0.000338 -0.002806 -0.002333  0.000484 -0.002035  0.001499  0.000388   \n",
       "4 -0.000837 -0.000333  0.000532 -0.000519  0.000791  0.000030  0.000158   \n",
       "\n",
       "         7         8         9     ...           65        66        67  \\\n",
       "0 -0.009784  0.003545  0.005625    ...     0.005500  0.007964 -0.009730   \n",
       "1  0.004532 -0.004600  0.004172    ...    -0.003803  0.005502  0.001468   \n",
       "2 -0.002289 -0.001002  0.000100    ...     0.000022 -0.002538  0.000222   \n",
       "3  0.000348  0.000585  0.001262    ...     0.000467 -0.003581 -0.001602   \n",
       "4  0.000155 -0.000722  0.001307    ...    -0.001045 -0.000203  0.000142   \n",
       "\n",
       "         68        69        70        71        72        73        74  \n",
       "0 -0.004119 -0.007043 -0.002126 -0.005411 -0.009777 -0.016776  0.007655  \n",
       "1  0.002383 -0.002151  0.002215  0.005457  0.005600  0.004212 -0.002091  \n",
       "2  0.002630  0.001108  0.003386  0.000761 -0.000951  0.003046  0.003184  \n",
       "3 -0.000692 -0.000991  0.000854  0.000137  0.000685  0.000358 -0.002380  \n",
       "4  0.000764  0.000128 -0.001107 -0.000260 -0.000582 -0.000172 -0.000032  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_pd.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sub_list to translate sr_id to row number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = pd.DataFrame(items_df.select('0').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table to translate sr_id to subreddit name\n",
    "- We will need this to generate final outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_lookup_df = auth_df.select(['subreddit', 'sr_id']).distinct()\n",
    "sr_lookup = sr_lookup_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finally, find subreddits using item-item similarity\n",
    "- This function returns the 10 most similar subreddits to a given input\n",
    "- Based on item-item similarity from the item-feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_set = set(sr_lookup.iloc[:,0])\n",
    "\n",
    "def item_item_sim(search_name):\n",
    "    if str(search_name) not in subreddit_set:\n",
    "        return \"Error: \" + str(search_name) + \" is not an active subreddit\"\n",
    "    \n",
    "    # Get subreddit id number and translate to row number\n",
    "    search_id = sr_lookup[sr_lookup['subreddit'] == str(search_name)].iloc[0,1]\n",
    "    search_row_num = sub_list[sub_list[0] == search_id].index[0]\n",
    "    \n",
    "    # Get item-feature row and calculate dot product\n",
    "    item_feature_np = np.asarray(items_pd.iloc[search_row_num, :])\n",
    "    item_dp = np.dot(item_feature_np, items_pd.T)\n",
    "    \n",
    "    # Get top ten similar rows, translate to subreddit name\n",
    "    sim_rows = list(item_dp.argsort())[-11:-1][::-1]\n",
    "    out_ids = sub_list.iloc[sim_rows]\n",
    "    out_names = list(out_ids.join(sr_lookup.set_index('sr_id'), on=0).loc[:,'subreddit'])\n",
    "    \n",
    "    return out_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fantasyfootball',\n",
       " 'CFB',\n",
       " 'nba',\n",
       " 'baseball',\n",
       " 'AskMen',\n",
       " 'hockey',\n",
       " 'Browns',\n",
       " 'quityourbullshit',\n",
       " 'food',\n",
       " 'hiphopheads']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_item_sim('nfl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
